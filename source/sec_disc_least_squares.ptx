<?xml version="1.0" encoding="UTF-8" ?>
<section xml:id="discrete-least-squares-approximation">
	<title>Discrete least squares approximation</title>

<p>
	Given the data in the vehicle speed problems (see
sections 3.3, 3.4 homework), we can plot a few functions representing
the position data (see Figure<xref ref="fig-speed1" />). The
Hermite (black) and spline (red) interpolations exactly pass through the
position data points. On the other hand, the linear approximation (blue)
does not exactly pass through a single of these points.
</p>

	<figure xml:id="fig-speed1">
	<image source="3_approximation/speed1.png"/>
	<caption>Data and interpolating and approximating
polynomials.</caption>
</figure>

	<figure xml:id="fig-speed2">
	<image source="3_approximation/speed2.png"/>
	<caption>Speed data and predicted speed by interpolating
and approximating polynomials (<term>left:</term> feet per second
and<term>right:</term> miles per hour).</caption>
</figure>

	<figure xml:id="fig-speed3">

	<image source="3_approximation/speed3.png"/>
	<caption>Speed data and predicted speed by interpolating
and approximating polynomials (<term>left:</term> feet per second
and<term>right:</term> miles per hour).</caption>
</figure>

<p>
	There are a variety of factors to weigh when choosing a
method for representing data by a function.
</p>

</section>

<!--><section xml:id="discrete-case-8_1-pg486">
<title>Discrete case (i.e., 8.1, pg. 486)</title>

<p>
	Consider the last term
<me>\sum_{i=1}^m\Big(P_n(x_i)\Big)^2 = \sum_{i=1}^m \left(\sum_{j=0}^n
a_j x_i^j\right)^2</me> in the definition of the error <m>E = E_2(a_0,
a_1, \dots, a_n)</m> for discrete least squares approximation. We have
the following (though I do not find this particularly illuminating and
instead prefer to differentiate before addressing the sums), where it
might initially be helpful to remember that <m>x_i^0 = 1</m> and
<m>x_i^1 = x_i</m> (e.g., <m>a_0 x_i^0 = a_0\cdot1 =a_0</m> and <m>a_1
x_i^1 = a_1x_i</m>), <me>\begin{aligned}
 \sum_{i=1}^m \left(\sum_{j=0}^n a_j x_i^j\right)^2 &amp; = \sum_{i=1}^m
(a_0 + a_1x_i+ \dots + a_n x_i^n)(a_0 + a_1x_i + \dots + a_n x_i^n)\\
 &amp; = \sum_{i=1}^m \Big[a_0 (a_0 + \dots + a_n x_i^n) + \dots +
a_jx_i^j(a_0 + \dots + a_n x_i^n) + \dots + a_n x_i^n(a_0 + \dots + a_n
x_i^n)\Big]\\
 &amp; = \sum_{i=1}^m \sum_{j=0}^n a_jx_i^j(a_0 + a_1x_i + \dots + a_n
x_i^n)\\
 &amp; = \sum_{j=0}^n a_j \sum_{i=1}^m x_i^j(a_0 + a_1x_i + \dots + a_n
x_i^n)\\
 &amp; = \sum_{j=0}^n a_j \sum_{i=1}^m x_i^j \sum_{k=0}^n a_k x_i^k\\
 &amp; = \sum_{j=0}^n a_j \sum_{i=1}^m \sum_{k=0}^n a_k x_i^{j+k}\\
% &amp; = \sum_{j=0}^n a_j \sum_{k=0}^n a_k \sum_{i=1}^m x_i^{j+k}\\
% &amp; = \sum_{i=1}^m \sum_{j=0}^n a_jx_i^j \sum_{k=0}^n a_kx_i^k\\
% &amp; = \sum_{i=1}^m a_0 a_0 + (a_0a_1 + a_1a_0)x_i +
%(a_0a_2+a_1a_1+a_2a_0)x_i^2 + \dots\\
% &amp; = \sum_{i=1}^m a_0 a_0 + (a_0a_1 + a_1a_0)x_i +
%(a_0a_2+a_1a_1+a_2a_0)x_i^2 + \dots\\
 &amp; = \sum_{j=0}^n\sum_{k=0}^n a_ja_k\Big(\sum_{i=1}^m x_i^{j+k}\Big)
 \end{aligned}</me> 
 
Our goal is the partial derivative of this term with
respect to <m>a_j</m>. Notice the following, where key steps are moving
differentiation under the sum, applying the product rule, and
rearranging the sum, <me>\begin{aligned}
\dfrac{\partial}{\partial a_j} \left(\sum_{i=1}^m \left(\sum_{j=0}^n a_j
x_i^j\right)^2 \right) &amp; = \dfrac{\partial}{\partial
a_j}\Big(\sum_{i=1}^m (a_0 + a_1x_i + \dots + a_n x_i^n)(a_0 + a_1x_i +
\dots + a_n x_i^n)\Big)\\
&amp; = \sum_{i=1}^m \dfrac{\partial}{\partial a_j}\Big((a_0 + a_1x_i +
\dots + a_n x_i^n)(a_0 + a_1x_i + \dots + a_n x_i^n)\Big)\\
&amp; = \sum_{i=1}^m \left(x_i^j(a_0 + a_1x_i + \dots + a_n x_i^n) +
(a_0 + a_1x_i + \dots + a_n x_i^n) x_i^j\right)\\
&amp; = \sum_{i=1}^m 2\left(x_i^j(a_0 + a_1x_i + \dots + a_n
x_i^n)\right)\\
&amp; = \sum_{i=1}^m 2\left(x_i^j\sum_{k=0}^n(a_kx_i^k)\right)\\
&amp; = 2\sum_{k=0}^n a_k \sum_{i=1}^m x_i^{j+k}\\
%&amp; = \int_a^b 2x_i^j(a_0 + a_1x_i + \dots + a_n x_i^n)\,dx_i\\
%&amp; = 2\int_a^b x_i^j\sum_{k=0}^n a_k x_i^k\,dx_i\\
%&amp; = 2\int_a^b \sum_{k=0}^n a_k x_i^{j+k}\,dx_i\\
%&amp; = 2\sum_{k=0}^n a_k \int_a^b x_i^{j+k}\,dx_i
\end{aligned}</me>

Related to this, to find the least squares polynomial approximation to
<m>\{(x_i, y_i)\}_{i=1}^m</m> by <m>P_n(x)~=~\sum\limits_{k=0}^n a_k
x^k</m>, we solve <me>D^TD\vec{a} =D^T\vec{y}</me> for the vector of
unknown coefficients <m>\vec{a}</m> where <me>D =
\left[\begin{array}{cccc} (x_1)^0 &amp; (x_1)^1 &amp; \cdots &amp;
(x_1)^n\\
(x_2)^0 &amp; (x_2)^1 &amp; \cdots &amp; (x_2)^n\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
(x_m)^0 &amp; (x_m)^1 &amp; \cdots &amp; (x_m)^n
\end{array}\right]</me>
</p>

</section>-->